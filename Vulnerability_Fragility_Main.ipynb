{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openseespy.opensees as ops\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import intersection as intrs\n",
    "import idealisasi as IP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input dan output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_pushover = \"D:/Belajar Open sees/Model 3D/Analisis Data/1. Data Pushover/2. Data pushover buku.csv\"                    # Folder path of pushover data         (Column 1 'Dt' in m  and column 2 'Vb' in kN)\n",
    "folder_adrs = \"D:/Belajar Open sees/Model 3D/Analisis Data/2. Data ADRS/ADRS02.csv\"                                           # Folder path of ADRS pushover data    (Column 1 floor, column 2 mass in ton, and column 3 mode)\n",
    "folder_GMR = \"D:/Belajar Open sees/Model 3D/Data Gempa\"                                                                       # Input Ground Motion file             (Column 1 constant time step in s and column 2 acceleration in g)\n",
    "file_loss_ratio = \"D:/Belajar Open sees/Model 3D/Analisis Data/3. Data loss ratio/Loss_ratio01.csv\"                           # Input Loass Ratio file (From d1 to ds3)  \n",
    "export_folder = 'D:/Belajar Open sees/Model 3D/Analisis Data/4. Output Data/Data 2'                                           # Folder output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Pushover Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pushover = pd.read_csv(folder_pushover)\n",
    "\n",
    "variabel_adrs = pd.read_csv(folder_adrs)\n",
    "max_PODISP = max(data_pushover.iloc[:, 0])\n",
    "\n",
    "plt.plot(data_pushover.iloc[:, 0], data_pushover.iloc[:, 1])\n",
    "plt.title('Pushover data')\n",
    "plt.ylabel('Base Sheer (kN)')\n",
    "plt.xlabel('Top Displacement (m)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushover data idealization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point1, point2, point3 = IP.SH(data_pushover, 0.001)                                                        # Idealizing pushover data in to bilinear with strain hardening and split into 3 damage state\n",
    "\n",
    "fy = point2\n",
    "fu = point3\n",
    "\n",
    "strain_y = fy[0]\n",
    "stress_y = fy[1]\n",
    "strain_u = fu[0]\n",
    "stress_u = fu[1]\n",
    "\n",
    "Epp = np.array([point1, point2, point3])\n",
    "plt.plot(data_pushover.iloc[:, 0], data_pushover.iloc[:, 1])\n",
    "plt.plot(Epp[:, 0], Epp[:, 1])\n",
    "plt.title('Pushover Data')\n",
    "plt.ylabel('Base Sheer (kN)')\n",
    "plt.xlabel('Top Displacement (m)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "ds = []\n",
    "ds.append(strain_y*0.75)\n",
    "ds.append((strain_y+strain_u)/2)\n",
    "ds.append(strain_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert pushover data to ADRS (Acceleration and displacement) format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variabel_adrs['normalisasi mode'] = variabel_adrs['mode'] / variabel_adrs.iloc[0, 2]\n",
    "\n",
    "mxmode = sum(variabel_adrs.iloc[:, 1]*variabel_adrs.iloc[:, 3])\n",
    "mxmode2 = sum(variabel_adrs.iloc[:, 1]*variabel_adrs.iloc[:, 3]*variabel_adrs.iloc[:, 3])\n",
    "tau = mxmode/mxmode2\n",
    "\n",
    "data_pushover['Sd'] = data_pushover['Dt'] / tau\n",
    "data_pushover['Sa'] = (data_pushover['Vb'] / 9.81) / (tau*mxmode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Ground Motion folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = os.listdir(folder_GMR)\n",
    "print(list_data)\n",
    "\n",
    "min_scale = 0.1\n",
    "max_scale = 2.0\n",
    "step = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdv_point = []                                                                                                  # Define nececeray variable for analysis and store the result\n",
    "pga_point = []\n",
    "\n",
    "g = 9.81                                                                                                        # gravity\n",
    "L = 1.0\n",
    "d = 1\n",
    "A = 1\n",
    "I3= (1/12)\n",
    "xDamp = 0.05                                                                                                    # damping default =  5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Spectrum Analysis + IDA\n",
    "note :  The analysis normalize each grpund motion pga to 1 g,\n",
    "        GMRS data scaling can be change on scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in list_data:\n",
    "    GMRS = pd.read_csv(folder_GMR+\"/\"+record,header=None)\n",
    "    dt = abs(GMRS.iloc[1, 0]-GMRS.iloc[0, 0])\n",
    "    nPts = len(GMRS)\n",
    "    GMRS_accl = GMRS.iloc[:, 1]*(1/max(GMRS.iloc[:, 1]))\n",
    "    state = 'not fail'\n",
    "    \n",
    "    for scale in np.arange(min_scale, max_scale+step, step):\n",
    "        GMRS_accl_scaled = GMRS_accl*scale \n",
    "        T =[]\n",
    "        Accl =[]\n",
    "        Disp = []\n",
    "        \n",
    "        if state == 'not fail':\n",
    "            #Response Spectrum\n",
    "            for Period in np.arange(0.01, 6.0, 0.01):\n",
    "\n",
    "                # model\n",
    "                ops.wipe()\n",
    "\n",
    "                ops.model('Basic', '-ndm', 2, '-ndf', 3)\n",
    "\n",
    "                K = stress_y/strain_y\n",
    "                E = K*L*L*L/I3\n",
    "\n",
    "                M = K*(Period**2)/4/(np.pi**2)\n",
    "                omega = np.sqrt(K/M)\n",
    "\n",
    "                ops.node(1, 0, 0)\n",
    "                ops.node(2, 0, 0)\n",
    "                ops.node(3, 0, L)\n",
    "\n",
    "                ops.mass(3, M, M, 0)\n",
    "\n",
    "                ops.fix(1, 1, 1, 1)\n",
    "\n",
    "                ops.equalDOF(1, 2, *[1, 2])\n",
    "\n",
    "                ops.geomTransf('Linear', 1, 0, 1)\n",
    "\n",
    "                ops.uniaxialMaterial('MultiLinear', 1, strain_y, stress_y, strain_u, stress_u)\n",
    "\n",
    "                ops.element('zeroLength', 1, 1, 2, '-mat', 1, '-dir', 6, '-doRayleigh', 1) \n",
    "\n",
    "                ops.element('elasticBeamColumn', 2, 2, 3, A, E, I3, 1)\n",
    "\n",
    "                # Set time series to be passed to uniform excitation\n",
    "                ops.timeSeries('Path', 2,  '-values', *GMRS_accl_scaled, '-dt', dt, '-factor', g, 'prependZero')\n",
    "\n",
    "                ops.pattern('UniformExcitation',  1,   1,  '-accel', 2, '-fact', 1)\n",
    "\n",
    "                betaKcomm \t= 2.0*xDamp/omega; \t \n",
    "                            \n",
    "                ops.rayleigh(0.0, 0.0, 0.0, betaKcomm)\n",
    "\n",
    "                ops.wipeAnalysis()\t\n",
    "                ops.system('BandSPD')\n",
    "                ops.constraints('Plain')\n",
    "                ops.test('NormDispIncr', 1.0e-12,  100, 0) \n",
    "                ops.algorithm('Newton')  \n",
    "                ops.numberer(\"RCM\")                     \n",
    "                ops.integrator('Newmark',  0.5,  0.25 )            \n",
    "                ops.analysis(\"Transient\")\n",
    "                dtAnalysis    = dt\n",
    "                TmaxAnanlysis = dt*nPts\n",
    "                tFinal        = TmaxAnanlysis\n",
    "                tCurrent      = ops.getTime()\n",
    "                ok            = 0\n",
    "                \n",
    "                # Initializations of response\n",
    "                u1            = []\n",
    "\n",
    "                while ok == 0 and tCurrent < tFinal:\n",
    "                    ok = ops.analyze(1, dtAnalysis)\n",
    "                    # if the analysis fails try initial tangent iteration\n",
    "                    if ok != 0:\n",
    "                        print(\"Iteration failed .. lets try an initial stiffness for this step\")\n",
    "                        ops.test('NormDispIncr', 1.0e-12,  100, 0)\n",
    "                        ops.algorithm('ModifiedNewton', '-initial')\n",
    "                        ok = ops.analyze( 1, .001)\n",
    "                        \n",
    "                        if ok == 0:\n",
    "                            print(\"that worked .. back to regular newton\")\n",
    "                            ops.test('NormDispIncr', 1.0e-12,  10 )\n",
    "                            ops.algorithm('Newton')    \n",
    "                    tCurrent = ops.getTime()\n",
    "                    u1.append(ops.nodeDisp(3, 1))\n",
    "\n",
    "                max_disp = max(abs(x) for x in u1)\n",
    "                Accl.append(max_disp*(omega*omega)/g)\n",
    "                Disp.append(max_disp)\n",
    "\n",
    "            rs = {'Sd': Disp,\n",
    "                'Sa': Accl}\n",
    "            rs = pd.DataFrame(rs)\n",
    "            \n",
    "            # Finding the significant intersection point\n",
    "            intersection_point = intrs.intersect(rs, data_pushover.iloc[:, 2:4])\n",
    "            \n",
    "            if intersection_point is None:\n",
    "                sdv_point.append(max_PODISP)\n",
    "                pga_point.append(max(GMRS_accl_scaled))\n",
    "                state = 'fail'\n",
    "            else:\n",
    "                sdv_point.append(intersection_point[0])\n",
    "                pga_point.append(max(GMRS_accl_scaled))\n",
    "        else: \n",
    "            sdv_point.append(max_PODISP)\n",
    "            pga_point.append(max(GMRS_accl_scaled))\n",
    "        \n",
    "        print(record, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure = {'Sd': sdv_point,\n",
    "    'PGA': pga_point}\n",
    "\n",
    "failure = pd.DataFrame(failure)      \n",
    "\n",
    "for i in range(1, 4):                                                                                               #analize fail state of each data\n",
    "    yn = []\n",
    "    if i == 1 or i == 2 :\n",
    "        for n in range(failure.shape[0]):\n",
    "            if failure.Sd[n] >= ds[i-1] and failure.Sd[n] < ds[i]:\n",
    "                yn.append(1)\n",
    "            else:\n",
    "                yn.append(0)\n",
    "    \n",
    "    if i == 3:\n",
    "        for n in range(failure.shape[0]):\n",
    "            if failure.Sd[n] >= ds[i-1]:\n",
    "                yn.append(1)\n",
    "            else:\n",
    "                yn.append(0)\n",
    "    failure['ds'+str(i)]=yn\n",
    "\n",
    "failure.to_csv(export_folder+'/'+'failure state.csv', index = None)                                                                        # Export analysis result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure = pd.read_csv(export_folder+'/'+'failure state.csv')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(failure['PGA'], failure['Sd'] , marker='o', color='b', label='Performannce point')\n",
    "\n",
    "plt.axhline(y=ds[0], color='r', linestyle='--', label='ds1')\n",
    "plt.axhline(y=ds[1], color='r', linestyle='--', label='ds2')\n",
    "plt.axhline(y=ds[2], color='r', linestyle='--', label='ds3')\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "plt.ylim(ymin=0)\n",
    "\n",
    "plt.ylabel('Spectral Displacement (Sd)')\n",
    "plt.xlabel('Peak Ground Accel (PGA)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fragility Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE with Multy Stripe Analysis (Baker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform optimization and plotting for each damage state\n",
    "def plot_fragility_curve(IM, damaged_building, total_building, label, color, IM_range):\n",
    "    # Convert data to binomial\n",
    "    y = damaged_building / total_building\n",
    "\n",
    "    # Define the log-likelihood function\n",
    "    def neg_loglik(theta):\n",
    "        mu = theta[0]\n",
    "        sigma = theta[1]\n",
    "        prob = norm.cdf((np.log(IM) - mu) / sigma)\n",
    "        ll = y * np.log(prob) + (1 - y) * np.log(1 - prob)\n",
    "        return -np.sum(ll)\n",
    "\n",
    "    # Perform optimization to estimate the parameters\n",
    "    theta_start = np.array([0, 1])\n",
    "    res = minimize(neg_loglik, theta_start, method='Nelder-Mead')\n",
    "\n",
    "    mu_hat, sigma_hat = res.x\n",
    "\n",
    "    # Calculate fragility curve\n",
    "    p_collapse_range = norm.cdf((np.log(IM_range) - mu_hat) / sigma_hat)\n",
    "\n",
    "    # Plotting the fragility curve\n",
    "    plt.plot(IM_range, p_collapse_range, label=f'{label} (mu={mu_hat:.2f}, sigma={sigma_hat:.2f})', color=color)\n",
    "    plt.scatter(IM, y, marker='x', color=color)\n",
    "    \n",
    "    return p_collapse_range, mu_hat, sigma_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.array(failure['PGA'])\n",
    "IM_range = np.linspace(0, np.max(failure['PGA']), 1000)\n",
    "\n",
    "y_values = np.array(failure['Sd'])\n",
    "IM = np.arange(min_scale, max_scale+step, step)\n",
    "num_gms = np.full_like(IM, len(list_data))\n",
    "num_real = [np.sum((x_values > i*0.99) & (x_values < i*1.01)) for i in IM]\n",
    "num_add = num_gms-num_real\n",
    "\n",
    "data = x_values[y_values >= ds[0]]\n",
    "num_collapse = [np.sum((data > i*0.99) & (data < i*1.01)) for i in IM]\n",
    "num_collapse = num_collapse+num_add\n",
    "num_collapse_ds1 = num_collapse\n",
    "\n",
    "data = x_values[y_values >= ds[1]]\n",
    "num_collapse = [np.sum((data > i*0.99) & (data < i*1.01)) for i in IM]\n",
    "num_collapse = num_collapse+num_add\n",
    "num_collapse_ds2 = num_collapse\n",
    "\n",
    "data = x_values[y_values >= ds[2]]\n",
    "num_collapse = [np.sum((data > i*0.99) & (data < i*1.01)) for i in IM]\n",
    "num_collapse = num_collapse+num_add\n",
    "num_collapse_ds3 = num_collapse\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "frag_ds1_MLE, mu_ds1, sigma_ds1 = plot_fragility_curve(IM, num_collapse_ds1, num_gms, 'DS1', 'Green', IM_range)\n",
    "frag_ds2_MLE, mu_ds2, sigma_ds2 = plot_fragility_curve(IM, num_collapse_ds2, num_gms, 'DS2', 'Yellow', IM_range)\n",
    "frag_ds3_MLE, mu_ds3, sigma_ds3 =  plot_fragility_curve(IM, num_collapse_ds3, num_gms, 'DS3', 'Red', IM_range)\n",
    "\n",
    "plt.title('Lognormal CDF Fragility Curves using MLE')\n",
    "plt.xlabel('PGA (g)')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Ratio input and Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ratio = pd.read_csv(file_loss_ratio, header=None)\n",
    "loss_ratio = loss_ratio.values\n",
    "print(loss_ratio)\n",
    "\n",
    "vul_MLE= []\n",
    "\n",
    "for i in range(len(IM_range)):\n",
    "    vul_MLE.append((frag_ds1_MLE[i]-frag_ds2_MLE[i])*loss_ratio[0]+(frag_ds2_MLE[i]-frag_ds3_MLE[i])*loss_ratio[1]+frag_ds3_MLE[i]*loss_ratio[2])\n",
    "\n",
    "plt.plot(x, vul_MLE, '-b', label='Vulnerability MLE')\n",
    "plt.title('Vulnerability Curve')\n",
    "plt.xlabel('PGA (g)')\n",
    "plt.ylabel('Loss Ratio')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragility_vulnerability = {\n",
    "    'IM':IM_range,\n",
    "    'Frag_DS1':frag_ds1_MLE,\n",
    "    'Frag_DS2':frag_ds2_MLE,\n",
    "    'Frag_DS3':frag_ds3_MLE,\n",
    "    'Vulnerability':vul_MLE\n",
    "}\n",
    "fragility_vulnerability.to_csv(export_folder+'/'+'Fragility_Vulnerability.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
